{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NLP demo using NLTK\n",
    "NLTK is a toolkit build for working with NLP in Python. It provides us various text processing libraries with a lot of test datasets.<br><br>\n",
    "Install nltk: pip install nltk <br><br>\n",
    "Blog: https://becominghuman.ai/nlp-for-beginners-using-nltk-f58ec22005cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text to lower Case\n",
    "It is necessary to convert the text to lower case as it is case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a demo text for nlp using nltk. full form of nltk is natural language toolkit\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "lower_text = text.lower()\n",
    "print (lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word tokenize\n",
    "Tokenize sentences to get the tokens of the text i.e breaking the sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'Demo', 'Text', 'for', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'of', 'NLTK', 'is', 'Natural', 'Language', 'Toolkit']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print (word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence tokenize\n",
    "Tokenize sentences if the there are more than 1 sentence i.e breaking the sentences to list of sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a Demo Text for NLP using NLTK.', 'Full form of NLTK is Natural Language Toolkit']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "sent_token = nltk.sent_tokenize(text)\n",
    "print (sent_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop words removal\n",
    "Remove irrelevant words using nltk stop words like is,the,a etc from the sentences as they donâ€™t carry any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Demo', 'Text', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'NLTK', 'Natural', 'Language', 'Toolkit']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "text = 'This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit'\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "removing_stopwords = [word for word in word_tokens if word not in stopword]\n",
    "print (removing_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemma\n",
    "lemmatize the text so as to get its root form eg: functions,funtionality as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'dog', 'are', 'barking', 'outside', '.', 'Are', 'the', 'cat', 'in', 'the', 'garden', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#is based on The Porter Stemming Algorithm\n",
    "stopword = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"the dogs are barking outside. Are the cats in the garden?\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print (lemmatized_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'demo', 'text', 'for', 'nlp', 'use', 'nltk', '.', 'full', 'form', 'of', 'nltk', 'is', 'natur', 'languag', 'toolkit']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "#is based on The Porter Stemming Algorithm\n",
    "stopword = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
    "print (stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get word frequency\n",
    "counting the word occurrence using FreqDist library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 2), ('nltk', 2), ('this', 1), ('a', 1), ('demo', 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "text = \"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "word = nltk.word_tokenize(text.lower())\n",
    "freq = FreqDist(word)\n",
    "print (freq.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  pos(Part of Speech)tags\n",
    "POS tag helps us to know the tags of each word like whether a word is noun, adjective etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('outside', 'IN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"the dogs are barking outside.\"\n",
    "word = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "print (pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER\n",
    "NER(Named Entity Recognition) is the process of getting the entity names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('who', 'WP'), ('is', 'VBZ'), ('Barrack', 'NNP'), ('Obama', 'NNP')]\n",
      "(S who/WP is/VBZ (PERSON Barrack/NNP Obama/NNP))\n",
      "['Barrack Obama']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"who is Barrack Obama\"\n",
    "word = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "print(pos_tag)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "print(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85ab11090a32b58d1dba88bd22dc30d9d16814845c03675a7b99eed0df35fc67"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
